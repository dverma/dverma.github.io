---
layout: post
title:  "The Big O"
date:   2017-02-13
excerpt: "The Big O - What, Why & When?"
tag:
- Big O
- algorithms
image: 'assets/blog_images/bigO.png'
comments: true
---
# The Big O - What, Why & When?

## What is The Big O?

#### *According to Wikipedia:*

> Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.
>
> Big O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation.
>
> The letter O is used because the growth rate of a function is also referred to as order of the function. A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function. Associated with big O notation are several related notations, using the symbols \(o, Ω, ω and  Θ \), to describe other kinds of bounds on asymptotic growth rates.

The Big O is the order of a function/algorithm \\(O(n)\\) where n is the size of input to the function/algorithm.



---

Let's see some common order of functions and a few examples:

### Constant - \\(O(1)\\)
Constant time is obviously the best of the lot! It really can't get better than this. What this really means is that no matter the input size, your algorithm will always take constant time. 

A couple of examples for functions with \\(O(1)\\) are: 

* A function that swaps the \\(i^{th}\\) & \\(j^{th}\\) elements in an array
* Inserting a node at the beginning in a Linked List

{% highlight java %}
// Example 1: Swap ith and jth element in an array
void swap(int[] arr, int i, int j)
{
    int temp = arr[i];
    arr[i] = arr[j];
    arr[j] = temp;
}
//Example 2: Inserting a node at the beginning in a Linked List
void insert(T data)
{
    if (head == null)
    {
        head = new Node<T>(data);
    }
    else
    {
        Node<T> newNode = new Node<T>(data);
        newNode.setNext(head);
        head = newNode;
    }
}
{% endhighlight %}

---

### Logarithmic - \\(O(log\\) \\(n)\\) 
Logarithmic time is usually found in operations on a binary tree and are highly efficient as at each iteration the operations are halved from the previous iteration. 

Binary Search is probably the most famous algorithm from this class which takes as input a sorted array and a number to search and returns whether the number is in the array or not. The idea is to start the search from the middle of an array and if the number to be searched is less then the number exists in the first half of the array otherwise in the second half. This process of halving the search size of array is repeated until we reach either the beginning or end of an array.

{% highlight java %}
boolean binarySearch(int[] arr, int n)
{
    int end = arr.length-1;
    int start = 0;
    int mid;
    if(arr[start]==n || arr[end]==n)
    	return true;
    if(n < arr[start] && n>arr[end])
    	return false;
    while(start < end-1)
    {
        mid = (start+end)/2;
        if(arr[mid]==n)
            return true;
        else if(arr[mid] > n)
            end = mid;
        else
            start = mid;
    }
    return false;
}
{% endhighlight %}

### Linear - \\(O(n)\\)

{% highlight java %}

{% endhighlight %}

### QuasiLinear - \\(O(n\\) \\({log^{k}}\\) \\(n)\\) \\(for\\) \\(some\\) \\(constant\\) \\(k\ge 1\\)

{% highlight java %}

{% endhighlight %}

### Polynomial - \\(O(n^{k})\\) \\(for\\) \\(some\\) \\(constant\\) \\(k>1\\)

{% highlight java %}

{% endhighlight %}

### Exponential - \\(O(2^{n^{k}})\\) \\(for\\) \\(some\\) \\(constant\\) \\(k\\)

{% highlight java %}

{% endhighlight %}



## Why is it important?
Big O notation is used in Computer Science to describe the performance or complexity of an algorithm; specifically the worst-case scenario(upper bound on the growth rate of the function), and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm. 
Worst case scenarios are of utmost importance because based on your input the time taken to compute a result for your algorithm can either be a few seconds or a few years, and we ofcourse want it to be a few seconds.

For ex: 

BubbleSort is \\(O(n^{2})\\), while merge sort is \\(O(n\\) \\({log}\\) \\(n)\\). In approximate terms, this means that sorting a million numbers would take around a trillion iterations through bubblesort  (1,000,000 * 1,000,000), while sorting through quicksort would take around  20 million (1,000,000 * 20). In other words, for a million numbers, quicksort would be about 50 times faster.

The following XKCD illustration best explains the importance of Big O: 
![Who better than Randall Munroe](http://imgs.xkcd.com/comics/1337_part_2.png)

## When is it important?
>  When is it not? :)

## Motivation for this post
In the corporate world making enterprise level software built upon frameworks that do most of the heavy lifting when it comes to the efficiency, we tend to ignore the basics. Well, it was time I touched upon those again.
